{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1f1eb47",
   "metadata": {},
   "source": [
    "# BERT: Bidirectional Encoder Representations from Transformers\n",
    "This lecture covers the BERT model, its architecture, and a hands-on example for using BERT in a Question Answering (Q&A) system.\n",
    "\n",
    "In this notebook, we will learn about BERT and implement a Q&A system using the Hugging Face `transformers` library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945e6a55",
   "metadata": {},
   "source": [
    "## What is BERT?\n",
    "- BERT is a **transformer-based model** developed by Google AI in 2018.\n",
    "- It stands for **Bidirectional Encoder Representations from Transformers**.\n",
    "- BERT is pre-trained on a large corpus of text and uses **bidirectional training** to better understand context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ead0cc",
   "metadata": {},
   "source": [
    "## Key Innovations of BERT\n",
    "1. **Bidirectional Context**: Unlike previous models, BERT reads text both **left-to-right** and **right-to-left** simultaneously.\n",
    "2. **Transformer Architecture**: Uses a transformer model with **self-attention** mechanisms.\n",
    "3. **Pre-training and Fine-tuning**: BERT is pre-trained on a large dataset and can be fine-tuned for specific NLP tasks like classification, Q&A, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81066de6",
   "metadata": {},
   "source": [
    "## BERT Model Architecture\n",
    "- Uses **stacked transformer layers**.\n",
    "- **Encoder-only model** (unlike GPT, which uses a decoder).\n",
    "- Each layer applies **self-attention** and **feed-forward networks**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a8467f",
   "metadata": {},
   "source": [
    "## How Does BERT Work?\n",
    "BERT uses two main pre-training tasks:\n",
    "1. **Masked Language Modeling (MLM)**: Randomly masks words in a sentence and the model tries to predict them.\n",
    "2. **Next Sentence Prediction (NSP)**: Determines if one sentence follows another."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc043ea",
   "metadata": {},
   "source": [
    "## Fine-Tuning BERT for NLP Tasks\n",
    "BERT can be fine-tuned for various NLP tasks:\n",
    "- **Question Answering (Q&A)**\n",
    "- **Sentiment Analysis**\n",
    "- **Named Entity Recognition (NER)**\n",
    "- **Text Classification**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e6d32a",
   "metadata": {},
   "source": [
    "## BERT for Question Answering\n",
    "- BERT can extract **answers from a given context**.\n",
    "- It does this by predicting the **start and end positions** of the answer span in the context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f87d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary libraries\n",
    "!pip install transformers torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e82d43",
   "metadata": {},
   "source": [
    "## Importing Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5eea5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForQuestionAnswering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae676b67",
   "metadata": {},
   "source": [
    "## Loading Pre-trained BERT Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092621b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained BERT model for Question Answering\n",
    "model_name = \"bert-large-uncased-whole-word-masking-finetuned-squad\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertForQuestionAnswering.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f204991",
   "metadata": {},
   "source": [
    "## Define Context and Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1a8116",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"\"\"\n",
    "OpenAI is an artificial intelligence research laboratory consisting of researchers and engineers. \n",
    "It was founded by Elon Musk, Sam Altman, and others with the mission to ensure that artificial general intelligence (AGI) benefits all of humanity. \n",
    "OpenAI has developed several state-of-the-art AI technologies, including the GPT series of models and ChatGPT, which are used for natural language understanding and generation.\n",
    "\"\"\"\n",
    "\n",
    "questions = [\n",
    "    \"Who founded OpenAI?\",\n",
    "    \"What is the mission of OpenAI?\",\n",
    "    \"What technologies has OpenAI developed?\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5705dee2",
   "metadata": {},
   "source": [
    "## Define Question Answering Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac6ccdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_question(question, context):\n",
    "    \"\"\"Given a question and context, return the answer using BERT.\"\"\"\n",
    "    inputs = tokenizer(question, context, return_tensors=\"pt\")\n",
    "    input_ids = inputs[\"input_ids\"].tolist()[0]\n",
    "    outputs = model(**inputs)\n",
    "    start_scores = outputs.start_logits\n",
    "    end_scores = outputs.end_logits\n",
    "\n",
    "    start_index = torch.argmax(start_scores)\n",
    "    end_index = torch.argmax(end_scores) + 1\n",
    "    answer = tokenizer.convert_tokens_to_string(\n",
    "        tokenizer.convert_ids_to_tokens(input_ids[start_index:end_index])\n",
    "    )\n",
    "    return answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2572b43a",
   "metadata": {},
   "source": [
    "## Running the Q&A System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda567f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for question in questions:\n",
    "    print(f\"Question: {question}\")\n",
    "    answer = answer_question(question, context)\n",
    "    print(f\"Answer: {answer}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c851dbb",
   "metadata": {},
   "source": [
    "## Advantages of BERT in Q&A\n",
    "- **Contextual Understanding**: BERTâ€™s bidirectional nature allows it to better understand the context.\n",
    "- **Pre-trained on Large Data**: Fine-tuning on specific datasets (like SQuAD) makes it highly effective for Q&A tasks.\n",
    "- **Versatile**: Can be fine-tuned for a wide range of NLP tasks beyond Q&A."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9472cf6",
   "metadata": {},
   "source": [
    "## Limitations of BERT\n",
    "- **Computationally Expensive**: Requires significant hardware resources (GPU/TPU).\n",
    "- **Long Context Limitation**: Standard BERT models have a maximum sequence length of 512 tokens.\n",
    "- **Not Always Perfect**: May struggle with ambiguous or complex questions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14102026",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "- BERT is a powerful transformer model that has transformed NLP.\n",
    "- Its bidirectional nature, combined with pre-training, allows it to understand text contextually.\n",
    "- Fine-tuning BERT for Q&A systems is straightforward and highly effective."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
